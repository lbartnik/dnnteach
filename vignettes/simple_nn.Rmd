---
title: "Simple NN"
author: "Lukasz A. Bartnik"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    base_format: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Simple NN}
  %\VignetteEngine{knitr::bookdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
library(knitr)

knitr::opts_chunk$set(collapse = TRUE, comment = NA, prompt = FALSE, echo = TRUE, cache = TRUE, fig.align = 'center')
```

```{r}
library(dnnteach)
library(ggplot2)
library(tibble)
```



What are the mechanics of optimizing a NN? Let's take a closer look at a very simple NN - in fact, so simple that some people wouldn't even call it a NN - and use it to gain a few insights into what exactly happens during a gradient-descent-based optimization.

Our NN will attempt to follow the outputs of a simple quadratic function (we will, thus, have a single input `x`):

$$f(x) = x^2$$ 
```{r}
tibble(x = seq(-5, 5, .1), `f(x)` = quadratic(x)) %>%
  ggplot() + geom_line(aes(x = x, y = `f(x)`))
```


We will start with a two-neuron NN. Since it is supposed to approximate $f(x)$ we will denote it as $\hat{f}(x)$:

$$
\begin{equation}
\hat{f}(x) = \sum_{i=1}^{2} \sigma(w_i x + b_i)
(\#eq:quadratic)
\end{equation}
$$
where $\sigma(x)$ is the sigmoid function:

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$
I said that our NN is unusually simple - and it is, as we sum the sigmoids without any weights. Thus, this NN has only four parameters: $(w_1, w_2, b_1, b_2)$!


The sigmoid function looks like this:

```{r}
tibble(x = seq(-6, 6, .1), y = sigma(x)) %>%
  ggplot() + geom_line(aes(x = x, y = y))
```

Since we have two of those, my first guess is that the ideal NN would take one as is and the other one reversed along the $x$ axis - like so:

```{r}
tibble(x = seq(-6, 6, .1), y1 = sigma(x-2), y2 = sigma(-x-2)) %>%
  ggplot() +
  geom_line(aes(x = x, y = y1)) +
  geom_line(aes(x = x, y = y2), color = 'red')
```

Once summed up according to Equation \@ref(eq:quadratic), we will get something that resembles a quadratic function, at least in the range $(-3, 3)$
(you might notice that this an equivalent of the following weight values: $w_1=1$, $w_2=-1$, $b_1=b_2=-2$):

```{r}
tibble(x = seq(-6, 6, .1), y = sigma(x-2) + sigma(-x-2)) %>%
  ggplot() + geom_line(aes(x = x, y = y))
```


So, what does it take for NN to arrive at this solution?

# Random NN weights

Typically, people initialize the weigths of their NNs randomly. Doing so right, though, can be an art and over the years there have been many guidelines how to that best.

We will do the same, but since we have only 4 weights in total, we will not pay to much attention to this step and simply use a Gaussian random variable: $\mathcal{N}(0, 1)$.

```{r}
set.seed(43)

w <- rnorm(2)
b <- rnorm(2)
```

Time to generate the training data set. We will pick $x$ uniformly in $(-2, 2)$ and add a little noise to the output.

```{r}
X <- runif(100, -2, 2)
Y <- quadratic(X) + rnorm(length(X), sd=.1)
```

We are ready to run our first gradient-based optimization! We will use learning rate of $\eta=0.01$ and run the optimization for $500$ epochs.

```{r}
trace <- optimize(X, Y, w, b, eta=0.002, epochs=500)
```

Let's take a closer look at the epoch-by-epoch trace:

```{r}
head(trace)
```

We can start by looking at the loss at the end of each epoch:

```{r}
ggplot(trace) + geom_line(aes(x = epoch, y = loss))
```

We can also plot the response of our NN at the beginning and at the end of the optimization:

```{r out.width='90%',fig.width=8,fig.height=5}
begin <- data_response(X, Y, nn_response, epoch = head(trace, 1))
end <- data_response(X, Y, nn_response, epoch = tail(trace, 1))

bind_rows(mutate(begin, epoch = 1), mutate(end, epoch = 100)) %>%
  ggplot() + geom_point(aes(x = x, y = y, color = type)) + facet_wrap(~epoch, nrow = 1)
```

OK! So the end-state does indeed seem like a better fit to our quadratic function. But why does it have only one slope? We expected to see two...

Let's take a look at what individual neurons look like in both these cases.

```{r out.width='90%',fig.width=8,fig.height=5}
begin <- data_neurons(X, epoch = head(trace, 1))
end <- data_neurons(X, epoch = tail(trace, 1))

bind_rows(mutate(begin, epoch = 1), mutate(end, epoch = 100)) %>%
  ggplot() + geom_line(aes(x = x, y = y, color = neuron)) + facet_wrap(~epoch, nrow = 1)
```



```


if (FALSE) {
  experiment(X, Y, epochs = 500, w = c(-2, 2))

  experiment(X, Y, epochs = 1000, D = 2, eta = 0.01)
}



if (FALSE) {
  ANS <- grid(X, Y, c(-1, -1))

  hist(log(ANS$Loss))

  pdf("by_Loss.pdf")
  ANS %>%
    mutate(ll = log(Loss)) %>%
    arrange(ll) %>%
    a_ply(1, function(ans) {
      print(result2(X, Y, ans) + ggtitle(ans$ll))
    })
  dev.off()

  ANS %>%
    mutate(ll = log(Loss)) %>%
    ggplot() +
    geom_point(aes(x = w1, y = w2, color = ll))

  ANS %>%
    mutate(ll = log(Loss)) %>%
    ggplot() +
    geom_point(aes(x = b1, y = b2, color = ll))

  classify100 <- function(w1, w2) {
    if (w1 > 3 && w2 < -3) return(1)
    if (w1 < -3 && w2 > 3) return(1)
    if (w1 < -3 && w2 < 1) return(2)
    if (w1 < 1 && w2 < -2) return(2)
    if (w1 > 2 && abs(w2) < .5) return(3)
    if (abs(w1) < .5 && w2 > 2) return(3)
    if (w1 > 2 && w2 > 2) return(4)
    return(5)
  }

  classify250 <- function(w1, w2) {
    if (w1 > 3 && w2 < -3) return(1)
    if (w1 < -3 && w2 > 3) return(1)
    if (w1 < -2 && w2 < -2) return(2)
    if (w1 < -3 && w2 < 1) return(3)
    if (w1 < 1 && w2 < -3) return(3)
    if (w1 > 2 && w2 > 2) return(4)
    #    if (w1 > 3 && abs(w2) < .5) return(4)
    #    if (abs(w1) < .5 && w2 > 3) return(4)
    return(6)
  }


  ANS2 <- ANS %>%
    ddply(.(no), function(run) {
      run$group <- classify250(tail(run, 1)$w1, tail(run, 1)$w2)
      run
    }) %>%
    mutate(group = as.factor(group))


  ANS2 %>%
    filter(stage == 'finish') %>%
    ggplot() +
    geom_point(aes(x = w1, y = w2, color = group))

  ANS2 %>%
    as_tibble %>%
    filter(i %% 10 == 0 | stage %in% c("finish", "start")) %>%
    ggplot() +
    geom_point(aes(x = w1, y = w2, color = group, size=stage), alpha = .1) +
    scale_size_manual(values=c(1.3, .05, 1.3)) +
    scale_shape_manual(values = c(21, 16, 2))

  ggsave("trace_250_dense.pdf", width = 12, height = 10)


}
```
